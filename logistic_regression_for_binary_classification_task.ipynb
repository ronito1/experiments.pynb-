{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression Project (Predict Ad click)\n",
        "\n",
        "Logisitc Regression is commonly used to estimate the probability that an instance belongs to a particular class. If the estimated probability that an instance is greater than 50%, then the model predicts that the instance belongs to that class 1, or else it predicts that it does not. This makes it a binary classifier. In this notebook we will look at the theory behind `Logistic Regression`  and use it to indicating whether or not a particular internet user clicked on an Advertisement. We will try to create a model that will predict whether or not they will click on an ad based off the features of that user.\n",
        "\n",
        "This data set contains the following features:\n",
        "\n",
        "* '`Daily Time Spent on Site`': consumer time on site in minutes\n",
        "* '`Age`': cutomer age in years\n",
        "* '`Area Income`': Avg. Income of geographical area of consumer\n",
        "* '`Daily Internet Usage`': Avg. minutes a day consumer is on the internet\n",
        "* '`Ad Topic Line`': Headline of the advertisement\n",
        "* '`City`': City of consumer\n",
        "* '`Male`': Whether or not consumer was male\n",
        "* '`Country`': Country of consumer\n",
        "* '`Timestamp`': Time at which consumer clicked on Ad or closed window\n",
        "* '`Clicked on Ad`': 0 or 1 indicated clicking on Ad"
      ],
      "metadata": {
        "id": "LgFjAo1djHcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_style('whitegrid')\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "\n",
        "\n",
        "data = pd.read_csv(\"/kaggle/input/advertisement-click-on-ad/advertising.csv\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-08T08:46:20.839906Z",
          "iopub.execute_input": "2023-02-08T08:46:20.840356Z",
          "iopub.status.idle": "2023-02-08T08:46:20.858928Z",
          "shell.execute_reply.started": "2023-02-08T08:46:20.840287Z",
          "shell.execute_reply": "2023-02-08T08:46:20.857716Z"
        },
        "trusted": true,
        "id": "BxS1RjRFjHcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-08T08:46:20.869970Z",
          "iopub.execute_input": "2023-02-08T08:46:20.870414Z",
          "iopub.status.idle": "2023-02-08T08:46:20.889325Z",
          "shell.execute_reply.started": "2023-02-08T08:46:20.870378Z",
          "shell.execute_reply": "2023-02-08T08:46:20.888048Z"
        },
        "trusted": true,
        "id": "LviVTURVjHcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-08T08:46:20.902796Z",
          "iopub.execute_input": "2023-02-08T08:46:20.903933Z",
          "iopub.status.idle": "2023-02-08T08:46:20.935564Z",
          "shell.execute_reply.started": "2023-02-08T08:46:20.903887Z",
          "shell.execute_reply": "2023-02-08T08:46:20.934667Z"
        },
        "trusted": true,
        "id": "AZLqNdAmjHcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "BQ7orRg0jHcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "data.Age.hist(bins=data.Age.nunique())\n",
        "plt.xlabel('Age')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-08T08:46:20.937157Z",
          "iopub.execute_input": "2023-02-08T08:46:20.938044Z",
          "iopub.status.idle": "2023-02-08T08:46:21.288822Z",
          "shell.execute_reply.started": "2023-02-08T08:46:20.937992Z",
          "shell.execute_reply": "2023-02-08T08:46:21.287340Z"
        },
        "trusted": true,
        "id": "VGtZ80WUjHcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.jointplot(x=data[\"Area Income\"], y=data.Age)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-08T08:46:21.290455Z",
          "iopub.execute_input": "2023-02-08T08:46:21.290906Z",
          "iopub.status.idle": "2023-02-08T08:46:21.950453Z",
          "shell.execute_reply.started": "2023-02-08T08:46:21.290862Z",
          "shell.execute_reply": "2023-02-08T08:46:21.949048Z"
        },
        "trusted": true,
        "id": "zCJT4oEgjHcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.jointplot(x=data[\"Daily Time Spent on Site\"], y=data.Age, kind='kde')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-08T08:46:21.952891Z",
          "iopub.execute_input": "2023-02-08T08:46:21.953277Z",
          "iopub.status.idle": "2023-02-08T08:46:23.147227Z",
          "shell.execute_reply.started": "2023-02-08T08:46:21.953224Z",
          "shell.execute_reply": "2023-02-08T08:46:23.145435Z"
        },
        "trusted": true,
        "id": "nZvkBHbyjHcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.jointplot(x=data[\"Daily Time Spent on Site\"], y=data[\"Daily Internet Usage\"])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-08T08:46:23.149345Z",
          "iopub.execute_input": "2023-02-08T08:46:23.149887Z",
          "iopub.status.idle": "2023-02-08T08:46:23.798143Z",
          "shell.execute_reply.started": "2023-02-08T08:46:23.149833Z",
          "shell.execute_reply": "2023-02-08T08:46:23.797093Z"
        },
        "trusted": true,
        "id": "t9qXzE2GjHcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(data, hue='Clicked on Ad')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-08T08:46:23.799698Z",
          "iopub.execute_input": "2023-02-08T08:46:23.800368Z",
          "iopub.status.idle": "2023-02-08T08:46:31.495518Z",
          "shell.execute_reply.started": "2023-02-08T08:46:23.800334Z",
          "shell.execute_reply": "2023-02-08T08:46:31.494451Z"
        },
        "trusted": true,
        "id": "NxTwhZd0jHcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Clicked on Ad'].value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-08T08:46:31.496908Z",
          "iopub.execute_input": "2023-02-08T08:46:31.497850Z",
          "iopub.status.idle": "2023-02-08T08:46:31.508627Z",
          "shell.execute_reply.started": "2023-02-08T08:46:31.497804Z",
          "shell.execute_reply": "2023-02-08T08:46:31.507195Z"
        },
        "trusted": true,
        "id": "HtGLQA-ZjHcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(data.corr(), annot=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-08T08:46:31.510446Z",
          "iopub.execute_input": "2023-02-08T08:46:31.510901Z",
          "iopub.status.idle": "2023-02-08T08:46:32.023757Z",
          "shell.execute_reply.started": "2023-02-08T08:46:31.510858Z",
          "shell.execute_reply": "2023-02-08T08:46:32.022684Z"
        },
        "trusted": true,
        "id": "dBFXuNkbjHcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Theory Behind Logistic Regression\n",
        "\n",
        "Logistic regression is the go-to linear classification algorithm for two-class problems. It is easy to implement, easy to understand and gets great results on a wide variety of problems, even when the expectations the method has for your data are violated.\n",
        "\n",
        "### Description\n",
        "\n",
        "#### Logistic Regression\n",
        "\n",
        "Logistic regression is named for the function used at the core of the method, the [logistic function](https://en.wikipedia.org/wiki/Logistic_function).\n",
        "\n",
        "The logistic function, also called the **`Sigmoid function`** was developed by statisticians to describe properties of population growth in ecology, rising quickly and maxing out at the carrying capacity of the environment. Itâ€™s an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits.\n",
        "\n",
        "$$\\frac{1}{1 + e^{-x}}$$\n",
        "\n",
        "$e$ is the base of the natural logarithms and $x$ is value that you want to transform via the logistic function."
      ],
      "metadata": {
        "id": "MmbcInGujHcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.linspace(-6, 6, num=1000)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x, (1 / (1 + np.exp(-x))))\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Sigmoid Function\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-08T08:46:32.026854Z",
          "iopub.execute_input": "2023-02-08T08:46:32.027612Z",
          "iopub.status.idle": "2023-02-08T08:46:32.249790Z",
          "shell.execute_reply.started": "2023-02-08T08:46:32.027565Z",
          "shell.execute_reply": "2023-02-08T08:46:32.248635Z"
        },
        "trusted": true,
        "id": "L4yW6kbZjHcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The logistic regression equation has a very similar representation like linear regression. The difference is that the output value being modelled is binary in nature.\n",
        "\n",
        "$$\\hat{y}=\\frac{e^{\\beta_0+\\beta_1x_1}}{1+\\beta_0+\\beta_1x_1}$$\n",
        "\n",
        "or\n",
        "\n",
        "$$\\hat{y}=\\frac{1.0}{1.0+e^{-\\beta_0-\\beta_1x_1}}$$\n",
        "\n",
        "$\\beta_0$ is the intecept term\n",
        "\n",
        "$\\beta_1$ is the coefficient for $x_1$\n",
        "\n",
        "$\\hat{y}$ is the predicted output with real value between 0 and 1. To convert this to binary output of 0 or 1, this would either need to be rounded to an integer value or a cutoff point be provided to specify the class segregation point.\n",
        "***\n",
        "### Learning the Logistic Regression Model\n",
        "\n",
        "The coefficients (Beta values b) of the logistic regression algorithm must be estimated from your training data. This is done using [maximum-likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation).\n",
        "\n",
        "Maximum-likelihood estimation is a common learning algorithm used by a variety of machine learning algorithms, although it does make assumptions about the distribution of your data (more on this when we talk about preparing your data).\n",
        "\n",
        "The best coefficients would result in a model that would predict a value very close to 1 (e.g. male) for the default class and a value very close to 0 (e.g. female) for the other class. The intuition for maximum-likelihood for logistic regression is that a search procedure seeks values for the coefficients (Beta values) that minimize the error in the probabilities predicted by the model to those in the data (e.g. probability of 1 if the data is the primary class).\n",
        "\n",
        "We are not going to go into the math of maximum likelihood. It is enough to say that a minimization algorithm is used to optimize the best values for the coefficients for your training data. This is often implemented in practice using efficient numerical optimization algorithm (like the Quasi-newton method).\n",
        "\n",
        "When you are learning logistic, you can implement it yourself from scratch using the much simpler gradient descent algorithm."
      ],
      "metadata": {
        "id": "HVhJyg4fjHcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n",
        "    if train:\n",
        "        pred = clf.predict(X_train)\n",
        "        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n",
        "        print(\"Train Result:\\n================================================\")\n",
        "        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n",
        "        print(\"_______________________________________________\")\n",
        "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
        "        print(\"_______________________________________________\")\n",
        "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n",
        "\n",
        "    elif train==False:\n",
        "        pred = clf.predict(X_test)\n",
        "        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n",
        "        print(\"Test Result:\\n================================================\")\n",
        "        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n",
        "        print(\"_______________________________________________\")\n",
        "        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
        "        print(\"_______________________________________________\")\n",
        "        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-08T08:46:32.251625Z",
          "iopub.execute_input": "2023-02-08T08:46:32.252503Z",
          "iopub.status.idle": "2023-02-08T08:46:32.264584Z",
          "shell.execute_reply.started": "2023-02-08T08:46:32.252453Z",
          "shell.execute_reply": "2023-02-08T08:46:32.262824Z"
        },
        "trusted": true,
        "id": "84idfDaDjHcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reasons of using scikit-learn (not pandas) for ML preprocessing:\n",
        "1. You can cross-validate the entire workflow.\n",
        "2. You can grid search model & preprocessing hyperparameters.\n",
        "3. Avoids adding new columns to the source DataFrame.\n",
        "4. Pandas lacks separate fit/transform steps to prevent data leakage."
      ],
      "metadata": {
        "id": "y3jKzvuPjHcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X = data.drop(['Timestamp', 'Clicked on Ad', 'Ad Topic Line', 'Country', 'City'], axis=1)\n",
        "y = data['Clicked on Ad']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# cat_columns = []\n",
        "num_columns = ['Daily Time Spent on Site', 'Age', 'Area Income', 'Daily Internet Usage', 'Male']\n",
        "\n",
        "\n",
        "ct = make_column_transformer(\n",
        "    (MinMaxScaler(), num_columns),\n",
        "    (StandardScaler(), num_columns),\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "X_train = ct.fit_transform(X_train)\n",
        "X_test = ct.transform(X_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-08T08:46:32.266516Z",
          "iopub.execute_input": "2023-02-08T08:46:32.267965Z",
          "iopub.status.idle": "2023-02-08T08:46:32.301819Z",
          "shell.execute_reply.started": "2023-02-08T08:46:32.267915Z",
          "shell.execute_reply": "2023-02-08T08:46:32.300762Z"
        },
        "trusted": true,
        "id": "Q-GQTOQqjHcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Prepare Data for Logistic Regression\n",
        "The assumptions made by logistic regression about the distribution and relationships in your data are much the same as the assumptions made in linear regression.\n",
        "\n",
        "Much study has gone into defining these assumptions and precise probabilistic and statistical language is used. My advice is to use these as guidelines or rules of thumb and experiment with different data preparation schemes.\n",
        "\n",
        "Ultimately in predictive modeling machine learning projects you are laser focused on making accurate predictions rather than interpreting the results. As such, you can break some assumptions as long as the model is robust and performs well.\n",
        "\n",
        "- **Binary Output Variable:** This might be obvious as we have already mentioned it, but logistic regression is intended for binary (two-class) classification problems. It will predict the probability of an instance belonging to the default class, which can be snapped into a 0 or 1 classification.\n",
        "- **Remove Noise:** Logistic regression assumes no error in the output variable (y), consider removing outliers and possibly misclassified instances from your training data.\n",
        "- **Gaussian Distribution:** Logistic regression is a linear algorithm (with a non-linear transform on output). It does assume a linear relationship between the input variables with the output. Data transforms of your input variables that better expose this linear relationship can result in a more accurate model. For example, you can use log, root, Box-Cox and other univariate transforms to better expose this relationship.\n",
        "- **Remove Correlated Inputs:** Like linear regression, the model can overfit if you have multiple highly-correlated inputs. Consider calculating the pairwise correlations between all inputs and removing highly correlated inputs.\n",
        "- **Fail to Converge:** It is possible for the expected likelihood estimation process that learns the coefficients to fail to converge. This can happen if there are many highly correlated inputs in your data or the data is very sparse (e.g. lots of zeros in your input data)."
      ],
      "metadata": {
        "id": "Qp301FsIjHcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Implimenting Logistic Regression in Scikit-Learn"
      ],
      "metadata": {
        "id": "imqVZpvhjHcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "lr_clf = LogisticRegression(solver='liblinear')\n",
        "lr_clf.fit(X_train, y_train)\n",
        "\n",
        "print_score(lr_clf, X_train, y_train, X_test, y_test, train=True)\n",
        "print_score(lr_clf, X_train, y_train, X_test, y_test, train=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-08T08:46:32.303549Z",
          "iopub.execute_input": "2023-02-08T08:46:32.304378Z",
          "iopub.status.idle": "2023-02-08T08:46:32.333958Z",
          "shell.execute_reply.started": "2023-02-08T08:46:32.304341Z",
          "shell.execute_reply": "2023-02-08T08:46:32.332768Z"
        },
        "trusted": true,
        "id": "t0U7atYNjHcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "rf_clf = RandomForestClassifier(n_estimators=1000)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "print_score(rf_clf, X_train, y_train, X_test, y_test, train=True)\n",
        "print_score(rf_clf, X_train, y_train, X_test, y_test, train=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-08T08:46:32.335263Z",
          "iopub.execute_input": "2023-02-08T08:46:32.335627Z",
          "iopub.status.idle": "2023-02-08T08:46:34.790466Z",
          "shell.execute_reply.started": "2023-02-08T08:46:32.335596Z",
          "shell.execute_reply": "2023-02-08T08:46:34.789165Z"
        },
        "trusted": true,
        "id": "v1Wb0CZ8jHcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Performance Measurement\n",
        "\n",
        "#### 1. Confusion Matrix\n",
        "- Each row: actual class\n",
        "- Each column: predicted class\n",
        "\n",
        "First row: Non-clicked Ads, the negative class:\n",
        "* 143 were correctly classified as Non-clicked Ads. **True negatives**.\n",
        "* Remaining 6 were wrongly classified as clicked Ads. **False positive**\n",
        "\n",
        "\n",
        "Second row: The clicked Ads, the positive class:\n",
        "* 3 were incorrectly classified as Non-clicked Ads. **False negatives**\n",
        "* 146 were correctly classified clicked Ads. **True positives**\n",
        "\n",
        "#### 2. Precision\n",
        "\n",
        "**Precision** measures the accuracy of positive predictions. Also called the `precision` of the classifier ==> `98.01%`\n",
        "\n",
        "$$\\textrm{precision} = \\frac{\\textrm{True Positives}}{\\textrm{True Positives} + \\textrm{False Positives}}$$\n",
        "\n",
        "#### 3. Recall\n",
        "\n",
        "`Precision` is typically used with `recall` (`Sensitivity` or `True Positive Rate`). The ratio of positive instances that are correctly detected by the classifier.\n",
        "\n",
        "$$\\textrm{recall} = \\frac{\\textrm{True Positives}}{\\textrm{True Positives} + \\textrm{False Negatives}}$$ ==> `96.10%`\n",
        "\n",
        "#### 4. F1 Score\n",
        "\n",
        "$F_1$ score is the harmonic mean of precision and recall. Regular mean gives equal weight to all values. Harmonic mean gives more weight to low values.\n",
        "\n",
        "\n",
        "$$F_1=\\frac{2}{\\frac{1}{\\textrm{precision}}+\\frac{1}{\\textrm{recall}}}=2\\times \\frac{\\textrm{precision}\\times \\textrm{recall}}{\\textrm{precision}+ \\textrm{recall}}=\\frac{TP}{TP+\\frac{FN+FP}{2}}$$ ==> `97.05%`\n",
        "\n",
        "The $F_1$ score favours classifiers that have similar precision and recall.\n",
        "\n",
        "#### 5. Precision / Recall Tradeoff\n",
        "\n",
        "Increasing precision reduced recall and vice versa"
      ],
      "metadata": {
        "id": "UMB4uCkDjHcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "\n",
        "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
        "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
        "    plt.plot(thresholds, recalls[:-1], \"g--\", label=\"Recall\")\n",
        "    plt.xlabel(\"Threshold\")\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.title(\"Precisions/recalls tradeoff\")\n",
        "\n",
        "\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_test, lr_clf.predict(X_test))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.subplot(2, 2, 1)\n",
        "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(precisions, recalls)\n",
        "plt.xlabel(\"Precision\")\n",
        "plt.ylabel(\"Recall\")\n",
        "plt.title(\"PR Curve: precisions/recalls tradeoff\");"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-08T08:46:34.791939Z",
          "iopub.execute_input": "2023-02-08T08:46:34.792314Z",
          "iopub.status.idle": "2023-02-08T08:46:35.274008Z",
          "shell.execute_reply.started": "2023-02-08T08:46:34.792270Z",
          "shell.execute_reply": "2023-02-08T08:46:35.272493Z"
        },
        "trusted": true,
        "id": "9fiTe45vjHcO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}